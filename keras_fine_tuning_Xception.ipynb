{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In our setup, we:\n",
    "- created a data/ folder\n",
    "- created train/ and validation/ subfolders inside data/\n",
    "- created cats/ and dogs/ subfolders inside train/ and validation/\n",
    "- put the cat pictures index 0-999 in data/train/cats\n",
    "- put the cat pictures index 1000-1400 in data/validation/cats\n",
    "- put the dogs pictures index 12500-13499 in data/train/dogs\n",
    "- put the dog pictures index 13500-13900 in data/validation/dogs\n",
    "So that we have 1000 training examples for each class, and 400 validation examples for each class.\n",
    "In summary, this is our directory structure:\n",
    "```\n",
    "data/\n",
    "    train3/\n",
    "        dog/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cat/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dog/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cat/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "```\n",
    "'''\n",
    "import pandas as pd\n",
    "from tqdm import tqdm   #进度条\n",
    "from PIL import Image\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.applications import *\n",
    "from keras.preprocessing.image import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.utils import *\n",
    "\n",
    "# path to the model weights files.\n",
    "weights_path = '../keras/examples/vgg16_weights.h5'\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 299, 299\n",
    "\n",
    "train_data_dir = 'data/train3'\n",
    "valid_data_dir = 'data/validation'\n",
    "test_data_dir= 'data/test'\n",
    "\n",
    "nb_train_samples = 19944\n",
    "nb_validation_samples = 4986\n",
    "#batch_size = 277  #19944/277=72  4986/277=18\n",
    "batch_size = 72   #19944/72=277  4986/72=69.25\n",
    "#batch_size = 16   #19944/72=277  4986/72=69.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "83689472/83683744 [==============================] - 3s 0us/step\n",
      "Found 19944 images belonging to 2 classes.\n",
      "Found 4986 images belonging to 2 classes.\n",
      "0 input_1\n",
      "1 lambda_1\n",
      "2 block1_conv1\n",
      "3 block1_conv1_bn\n",
      "4 block1_conv1_act\n",
      "5 block1_conv2\n",
      "6 block1_conv2_bn\n",
      "7 block1_conv2_act\n",
      "8 block2_sepconv1\n",
      "9 block2_sepconv1_bn\n",
      "10 block2_sepconv2_act\n",
      "11 block2_sepconv2\n",
      "12 block2_sepconv2_bn\n",
      "13 conv2d_1\n",
      "14 block2_pool\n",
      "15 batch_normalization_1\n",
      "16 add_1\n",
      "17 block3_sepconv1_act\n",
      "18 block3_sepconv1\n",
      "19 block3_sepconv1_bn\n",
      "20 block3_sepconv2_act\n",
      "21 block3_sepconv2\n",
      "22 block3_sepconv2_bn\n",
      "23 conv2d_2\n",
      "24 block3_pool\n",
      "25 batch_normalization_2\n",
      "26 add_2\n",
      "27 block4_sepconv1_act\n",
      "28 block4_sepconv1\n",
      "29 block4_sepconv1_bn\n",
      "30 block4_sepconv2_act\n",
      "31 block4_sepconv2\n",
      "32 block4_sepconv2_bn\n",
      "33 conv2d_3\n",
      "34 block4_pool\n",
      "35 batch_normalization_3\n",
      "36 add_3\n",
      "37 block5_sepconv1_act\n",
      "38 block5_sepconv1\n",
      "39 block5_sepconv1_bn\n",
      "40 block5_sepconv2_act\n",
      "41 block5_sepconv2\n",
      "42 block5_sepconv2_bn\n",
      "43 block5_sepconv3_act\n",
      "44 block5_sepconv3\n",
      "45 block5_sepconv3_bn\n",
      "46 add_4\n",
      "47 block6_sepconv1_act\n",
      "48 block6_sepconv1\n",
      "49 block6_sepconv1_bn\n",
      "50 block6_sepconv2_act\n",
      "51 block6_sepconv2\n",
      "52 block6_sepconv2_bn\n",
      "53 block6_sepconv3_act\n",
      "54 block6_sepconv3\n",
      "55 block6_sepconv3_bn\n",
      "56 add_5\n",
      "57 block7_sepconv1_act\n",
      "58 block7_sepconv1\n",
      "59 block7_sepconv1_bn\n",
      "60 block7_sepconv2_act\n",
      "61 block7_sepconv2\n",
      "62 block7_sepconv2_bn\n",
      "63 block7_sepconv3_act\n",
      "64 block7_sepconv3\n",
      "65 block7_sepconv3_bn\n",
      "66 add_6\n",
      "67 block8_sepconv1_act\n",
      "68 block8_sepconv1\n",
      "69 block8_sepconv1_bn\n",
      "70 block8_sepconv2_act\n",
      "71 block8_sepconv2\n",
      "72 block8_sepconv2_bn\n",
      "73 block8_sepconv3_act\n",
      "74 block8_sepconv3\n",
      "75 block8_sepconv3_bn\n",
      "76 add_7\n",
      "77 block9_sepconv1_act\n",
      "78 block9_sepconv1\n",
      "79 block9_sepconv1_bn\n",
      "80 block9_sepconv2_act\n",
      "81 block9_sepconv2\n",
      "82 block9_sepconv2_bn\n",
      "83 block9_sepconv3_act\n",
      "84 block9_sepconv3\n",
      "85 block9_sepconv3_bn\n",
      "86 add_8\n",
      "87 block10_sepconv1_act\n",
      "88 block10_sepconv1\n",
      "89 block10_sepconv1_bn\n",
      "90 block10_sepconv2_act\n",
      "91 block10_sepconv2\n",
      "92 block10_sepconv2_bn\n",
      "93 block10_sepconv3_act\n",
      "94 block10_sepconv3\n",
      "95 block10_sepconv3_bn\n",
      "96 add_9\n",
      "97 block11_sepconv1_act\n",
      "98 block11_sepconv1\n",
      "99 block11_sepconv1_bn\n",
      "100 block11_sepconv2_act\n",
      "101 block11_sepconv2\n",
      "102 block11_sepconv2_bn\n",
      "103 block11_sepconv3_act\n",
      "104 block11_sepconv3\n",
      "105 block11_sepconv3_bn\n",
      "106 add_10\n",
      "107 block12_sepconv1_act\n",
      "108 block12_sepconv1\n",
      "109 block12_sepconv1_bn\n",
      "110 block12_sepconv2_act\n",
      "111 block12_sepconv2\n",
      "112 block12_sepconv2_bn\n",
      "113 block12_sepconv3_act\n",
      "114 block12_sepconv3\n",
      "115 block12_sepconv3_bn\n",
      "116 add_11\n",
      "117 block13_sepconv1_act\n",
      "118 block13_sepconv1\n",
      "119 block13_sepconv1_bn\n",
      "120 block13_sepconv2_act\n",
      "121 block13_sepconv2\n",
      "122 block13_sepconv2_bn\n",
      "123 conv2d_4\n",
      "124 block13_pool\n",
      "125 batch_normalization_4\n",
      "126 add_12\n",
      "127 block14_sepconv1\n",
      "128 block14_sepconv1_bn\n",
      "129 block14_sepconv1_act\n",
      "130 block14_sepconv2\n",
      "131 block14_sepconv2_bn\n",
      "132 block14_sepconv2_act\n",
      "133 global_average_pooling2d_1\n",
      "134 dropout_1\n",
      "135 dense_1\n"
     ]
    }
   ],
   "source": [
    "# build the VGG16 network\n",
    "\n",
    "#构造模型\n",
    "x_input = Input((299, 299, 3))\n",
    "x_input = Lambda(xception.preprocess_input)(x_input)\n",
    "\n",
    "base_model = Xception(input_tensor=x_input, weights='imagenet', include_top=False, pooling = 'avg')\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = Dropout(0.5)(base_model.output)\n",
    "x = Dense(1, activation='sigmoid',kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "model = Model(base_model.input, x)\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=90,\n",
    "                        width_shift_range=0.2,\n",
    "                        height_shift_range=0.2,\n",
    "                        shear_range=0.2,\n",
    "                        zoom_range=0.2,\n",
    "                        horizontal_flip=True)\n",
    "val_gen = ImageDataGenerator()\n",
    "train_generator = gen.flow_from_directory(train_data_dir, (299, 299), shuffle=True, \n",
    "                                          batch_size=64,class_mode='binary')\n",
    "valid_generator = val_gen.flow_from_directory(valid_data_dir, (299, 299), shuffle=True, \n",
    "                                          batch_size=32,class_mode='binary')\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    print(i,model.layers[i].name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "277/277 [==============================] - 610s 2s/step - loss: 0.2514 - acc: 0.9166 - val_loss: 0.1167 - val_acc: 0.9742\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.11671, saving model to xception-best_weight_freeze.h5\n",
      "Epoch 2/5\n",
      "277/277 [==============================] - 448s 2s/step - loss: 0.1530 - acc: 0.9457 - val_loss: 0.1059 - val_acc: 0.9746\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.11671 to 0.10594, saving model to xception-best_weight_freeze.h5\n",
      "Epoch 3/5\n",
      "277/277 [==============================] - 443s 2s/step - loss: 0.1383 - acc: 0.9512 - val_loss: 0.0901 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10594 to 0.09011, saving model to xception-best_weight_freeze.h5\n",
      "Epoch 4/5\n",
      "277/277 [==============================] - 444s 2s/step - loss: 0.1378 - acc: 0.9497 - val_loss: 0.0911 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.09011\n",
      "Epoch 5/5\n",
      "277/277 [==============================] - 443s 2s/step - loss: 0.1393 - acc: 0.9513 - val_loss: 0.0916 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.09011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8cc27a32e8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#训练模型并保存在验证集上损失函数最小的权重\n",
    "filepath=\"xception-best_weight_freeze.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_weights_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=nb_train_samples//batch_size,\n",
    "        epochs=5,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=nb_validation_samples//batch_size,\n",
    "        callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_on_xception(n, width, heigth, test_data_dir, model, weight, output_name):\n",
    "    x_test = np.zeros((n,width,heigth,3),dtype=np.uint8)\n",
    "\n",
    "    for i in tqdm(range(n)):\n",
    "    #for i in range(n):\n",
    "        img = load_img(test_data_dir+\"/test/\"+'/%d.jpg' % (i+1)) \n",
    "        x_test[i,:,:,:] = img_to_array(img.resize((width,heigth),Image.ANTIALIAS))\n",
    "    \n",
    "#     x_test = xception.preprocess_input(x_test)\n",
    "    model.load_weights(weight)\n",
    "    y_test = model.predict(x_test, verbose=1)\n",
    "    y_test = y_test.clip(min=0.005, max=0.995)\n",
    "    \n",
    "    df = pd.read_csv(\"sample_submission.csv\")\n",
    "    for i in tqdm(range(n)):\n",
    "        df.set_value(i, 'label', y_test[i])\n",
    "    df.to_csv(output_name, index=None)\n",
    "    df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 4873/12500 [00:33<00:52, 145.79it/s]"
     ]
    }
   ],
   "source": [
    "predict_on_xception(12500, 299, 299, test_data_dir, model, \"xception-best_weight_freeze.h5\", \"pred-xception-freeze.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compile the model with a SGD/momentum optimizer\n",
    "# and a very slow learning rate.\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# prepare data augmentation configuration\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "#model.fit_generator(\n",
    "#    train_generator,\n",
    "#    samples_per_epoch=nb_train_samples,\n",
    "#    epochs=epochs,\n",
    "#    validation_data=validation_generator,\n",
    "#    nb_val_samples=nb_validation_samples)\n",
    "\n",
    "# fine-tune the model\n",
    "epochs = 10\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples//batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_val_sample, _ = next(validation_generator)\n",
    "#y_pred = model.predict(X_val_sample)\n",
    "\n",
    "y_pred = base_model.predict(X_val_sample)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(X_val_sample))\n",
    "print (y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "\n",
    "nb_sample = 10\n",
    "\n",
    "for x, y in zip(X_val_sample[:nb_sample], y_pred.flatten()[:nb_sample]):\n",
    "    s = pd.Series({'Cat': 1-y, 'Dog': y})\n",
    "    axes = s.plot(kind='bar')\n",
    "    axes.set_xlabel('Class')\n",
    "    axes.set_ylabel('Probability')\n",
    "    axes.set_ylim([0, 1])\n",
    "    plt.show()\n",
    "\n",
    "    img = array_to_img(x)\n",
    "    display(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
